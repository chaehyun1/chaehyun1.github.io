---
layout: archive
title: "One parameter model - binomial / poisson"
categories:
  - bayes
use_math: true
---


## Week 2  

<br>교재: FCB  
### <3. One-parameter model>

<br>**1. Binomial Model**  
================================

Happiness data를 통해서 알아보기
--------------------------------

\begin{equation}  
n = 129, \quad Y_i = \begin{cases}
      1 & \text{if happy}  \\\\ 
      0 & \text{otherwise}
      \end{cases}  
\nonumber
\end{equation}

- Conditional on $\theta$, the $Y_i$'s are i.i.d. binary random variables with expectation $\theta$.  
<span style="color:blue">
\begin{equation}
p(y_1,...,y_{129}|\theta) = \theta^{\sum_{i=1}^{129}y_i}(1-\theta)^{129-\sum_{i=1}^{129}{y_i}}
\nonumber
\end{equation}
</span>  
  
<br>
- 만약 $Y_i = 1$이 118명, $Y_i = 0$이 11명이면?  
\begin{equation}
p(y_1,...,y_{129}|\theta) = \theta^{118}(1-\theta)^{11}
\nonumber
\end{equation}  

<br>
- $\theta$ is unknown number between 0 and 1.  
\begin{equation}
p(\theta) = 1 \text{ for all } \theta \in [0,1]
\nonumber
\end{equation}

<br>Suppose our prior as uniform distribution.  
\begin{equation}  
p(\theta|y_1,...y_{129}) = \frac{p(y_1,...,y_{129}|\theta)p(\theta)}{p(y_1,...,y_{129})}
\nonumber  
= p(y_1,...,y_{129}|\theta) \times \frac{1}{p(y_1,...,y_{129})} \propto p(y_1,...,y_{129}|\theta)  
\nonumber
\end{equation}

$p(\theta|y_1,...y_{129})$ and $p(y_1,...,y_{129}|\theta)$ are proportional to each other as functions of $\theta$. 
These two functions of $\theta$ have the same shape, but not necessarily the same scale.  

<br>We can calculate the scale or normalizing constant $\frac{1}{p(y_1,...,y_{129})}$ using $\int \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1} d\theta = 1$.  
Use $\int_0^1 p(\theta|y_1,...y_{129}) d\theta = 1$.  
Use $p(\theta|y_1,...y_{129}) = \theta^{118}(1-\theta)^{11}/p(y_1,...,y_{129})$.  

Thus, we can get $p(y_1,...,y_{129}) = \frac{\Gamma(119)\Gamma(112)}{\Gamma(131)}$  

<br>다시 베이즈 rule에 plug in!  
\begin{equation}
p(\theta|y_1,...y_{129}) = \frac{\Gamma(131)}{\Gamma(119)\Gamma(12)}\theta^{119-1}(1-\theta)^{12-1}  
\nonumber
\end{equation}  
이것은 beta(119,12)를 따른다.  

<br><br>Posterior inference under a uniform prior
------  
If $Y_1,...Y_n$ are i.i.d. binary($\theta$),  
<span style="color:blue">
\begin{equation}
p(\theta|y_1,...y_n) = \theta^{\sum{y_i}}(1-\theta)^{n - \sum{y_i}} \times p(\theta)/p(y_1,...y_n)
\nonumber
\end{equation}
</span>  

<br>
\begin{equation}
Pr(\theta \in A|Y_1 = y_1,...,Y_n = y_n) = Pr(\theta \in A|\sum_{i=1}^{n}{Y_i} = \sum_{i=1}^{n}{y_i})  
\nonumber
\end{equation}
$\sum_{i=1}^{n}{Y_i}$ contains all the information about $\theta$ from the data.  
$\sum_{i=1}^{n}{Y_i}$ = **"sufficient statistic"** for $\theta$ and $p(y_1,...y_n|\theta)$.  
It is sufficient to know $\sum_{i=1}^{n}{Y_i}$ in order to make inference about $\theta$.  
The "sufficient statistic $Y$" $= \sum_{i=1}^{n}{Y_i}$ has a binomial distribution with parameters $(n,\theta)$ where $Y_1,...,Y_n|\theta$ are i.i.d binary($\theta$) random variables.  

<br>The binomial distribution
------  
A random variable $Y \in$ {0,1,...,n} has a binomial$(n, \theta)$ distribution if  
\begin{equation}
Pr(Y=y|\theta) =  {n\choose y} \theta^{y}(1-\theta)^{n-y}, \quad y \in {0,1,...,n}
\nonumber
\end{equation}  

<br>
\begin{equation}
p(\theta|y) = \frac{p(y|\theta)p(\theta)}{p(y)} = {n\choose y} \frac{\theta^y(1-\theta)^{n-y}p(\theta)}{p(y)} = c(y)\theta^y(1-\theta)^{n-y}p(\theta)
\nonumber
\end{equation}  
- $c(y)$ is a function of $y$ and not of $\theta$.
- $c(y)$를 구해야 한다!  

<br><br>$c(y)$를 구하기 위해서는..    $\quad p(\theta) = 1$  
\begin{equation}
1 =\int_{0}^{1}{c(y)\theta^y(1-\theta)^{n-y}d\theta} \quad \rightarrow \quad 1 = c(y)\int_{0}^{1}{\theta^y(1-\theta)^{n-y}d\theta}
\nonumber
\end{equation}  
\begin{equation}
\rightarrow \ 1 = c(y)\frac{\Gamma(y+1)\Gamma(n-y+1)}{\Gamma(n+2)} \quad \rightarrow \quad c(y) = \frac{\Gamma(n+2)}{\Gamma(y+1)\Gamma(n-y+1)}  
\nonumber
\end{equation}  
    
<br>베이즈 정리에 집어넣기  
\begin{equation}
p(\theta|y) = \frac{\Gamma(n+2)}{\Gamma(y+1)\Gamma(n-y+1)}\theta^y(1-\theta)^{n-y} =\frac{\Gamma(n+2)}{\Gamma(y+1)\Gamma(n-y+1)}\theta^{(y+1)-1}(1-\theta)^{(n-y+1)-1} \nonumber
\end{equation}  
$= beta(y+1,n-y+1)$  

<br>Happiness example로 다시 보자. n = 129이고,  $Y \equiv \sum{Y_i} = 118$에서 알 수 있듯이 Y는 sufficient statistic이다.  
따라서 $p(\theta|y) = p(\theta|y_1,...,y_n) = beta(119,12)$  

<br><br>Posterior distributions under beta prior distributions
------------  
The uniform prior distribution = $beta(1,1)$  
$\{\theta|Y = y\} \sim beta(1 + y, 1 + n - y)$  

<br>
Suppose $\theta \sim beta(a,b)$ and $Y|\theta \sim binomial(n, \theta)$.  
\begin{equation}
p(\theta|y) = \frac{p(\theta)p(y|\theta)}{p(y)}
= \frac{1}{p(y)} \times \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\theta^{a-1}(1-\theta)^{b-1} \times {n\choose y} \theta^{y}(1-\theta)^{n-y}  \\
= c(n,y,a,b) \times \theta^{a+y-1}(1-\theta)^{b+n-y-1}  
\nonumber
\end{equation}  
 = beta(a+y, b+n-y)

<br>Conjugacy: posterior가 prior와 같은 분포 계열에 속함.  
When we use the Beta distribution as a prior, a posterior of binomial likelihood will also follow the beta distribution.  
$\theta|\{Y = y\} \sim beta(a+y, b+n-y)$  
\begin{equation}
E[\theta|y] = \frac{a+y}{a+b+n} = \frac{a+b}{a+b+n}\frac{a}{a+b} + \frac{n}{a+b+n} \frac{y}{n} = \frac{a+b}{a+b+n} \times \text{prior expectation} + \frac{n}{a+b+n} \times \text{data average}
\nonumber
\end{equation}  
- The posterior expectation is weighted average of prior expectation and the sample mean.    

<br><br>predictive distribution
----------------------  
<The predictive distribution of $\tilde{Y}$>  
$\tilde{Y}|\{y_1, ..., y_n}$: conditionally i.i.d. binary variables  
\begin{equation}
Pr(\tilde{Y}=1|{y_1, ..., y_n}) = \int Pr(\tilde{Y}=1, \theta|{y_1, ..., y_n}) d\theta = \int Pr(\tilde{Y}=1|\theta, {y_1, ..., y_n})p(\theta|{y_1, ..., y_n})d\theta = \int \theta p(\theta|{y_1, ..., y_n})d\theta
\nonumber
\end{equation}
\begin{equation}
= E[\theta|{y_1, ..., y_n}] = \frac{a + \sum_{i=1}^{n}y_i}{a+b+y}
\nonumber
\end{equation}  
- The predictive distribution depends on our observed data.
- $\tilde{Y}$ is not independent of $Y_1,...,Y_n$.  

<br><br>**1. Poisson Model**  
=============================
측정한 값이 정수일 때, 가장 간단한 확률 모델은 'Poisson' 모델이다.  
$Pr(Y=y|\theta) = \theta^y e^{-\theta} / y!$ for $y \in \{0,1,2,...\}$  
<br>
$E[Y|\theta] = \theta$  
$Var[Y|\theta] = \theta $  

<br><br>Posterior inference
----------------
$Y_1,...,Y_n$: i.i.d. Poisson with mean $\theta$, then the joint pdf of sample data is
\begin{equation}
Pr(Y_1 = y_1, ..., Y_n = y_n|\theta) = \sum_{i=1}^{n} p(y_i|\theta) = \sum_{i=1}^{n} \frac{1}{y_i!} \theta^y_i e^{-\theta}
\nonumber
\end{equation}
\begin{equation}
c(y_1, ..., y_n) = \theta^{\sum y_i} e^{-n\theta}
\nonumber
\end{equation}  
<br>
A class of prior densities is conjugate for a sampling model $p(y_1, ..., y_n|\theta)$ if the posterior is also in the class.
\begin{equation}
p(y_1, ..., y_n|\theta) \propto p(\theta) \theta^{\sum y_i} e^{-n\theta}
\nonumber
\end{equation}
위를 살펴보면, $\theta^{c_1} e^{-c_2 \theta}$의 형태가 포함된 것을 알 수 있다. 이러한 형태의 가장 간단한 class는 '감마 분포'족이다.   
<br>
$\theta$가 gamma(a,b)를 따른다면, 
\begin{equation}
p(\theta) = \frac{b^a}{\Gamma(a)} \theta^{a-1} e^{-b\theta} \quad \text{for } \theta, a, b > 0
\nonumber
\end{equation}
$E[\theta] = a/b$  
$Var[\theta] = a/b^2$  
$mode[\theta] = (a-1)/b$ if $a>=1$ or $0$ if $a<=1$  

<br>
Suppose $Y_1,...,Y_n|\theta$: i.i.d. poisson($\theta$) and $p(\theta)$ = gamma(a, b)
\begin{equation}
p(\theta|y_1, ..., y_n) = p(\theta) p(y_1, ..., y_n|\theta) / p(y_1, ..., y_n)
\nonumber
\end{equation}
\begin{equation}
= \{\theta^{a-1} e^{-b\theta} \} \times \theta^{\sum{y_i}} e^{-n\theta} \times c(y_1, ..., y_n, a,b)
\nonumber
\end{equation}
\begin{equation}
= \theta^{a + \sum{y_i} -1} e^{-(b+n)\theta} \times c(y_1, ..., y_n, a,b)
\nonumber
\end{equation}
This is evidently a gamma dist, and we have confirmed the 'conjugacy' of the gamma family for the Poisson sampling model:
\begin{equation}
\theta \sim gamma(a, b) \quad \text{and} \quad Y_1, ..., Y_n|\theta \sim Poisson(\theta) \rightarrow \{ \theta|Y_1, ..., Y_n \} \sim gamma(a + \sum_{y_i=2}^{n}Y_i, b+n)
\nonumber
\end{equation}

<br>
