---
layout: archive
title: "One parameter model + Introduction to multiparameter models"

use_math: true
---


## Week 3  

<br>교재: BDA  

Posterior as compromise between data and prior information
----------------------
베이지안 추론의 과정은 prior distribution에서 posterior distribution으로 나아가는 것을 포함한다. 사후 분포는 데이터의 정보를 포함하기 때문에, 사전 분포보다 덜 가변적이다. 이것은 다음과 같이 나타낼 수 있다.  
\begin{equation}
E(\theta) = E(E(\theta|y))
\end{equation}  
and  
\begin{equation}
var(\theta) = E(var(\theta|y)) + var(E(\theta|y))
\end{equation}  
(1)은 $\theta$의 사전 평균은 모든 가능한 posterior means의 평균이다.   
(2)는 posterior variance의 평균은 prior variance보다 작다는 것을 보여준다.  

<br><br>Estimating a normal mean with known variance
=====================

Likelihood of one data point
--------------------
y: a single scalar observation from a normal distribution(mean: $\theta$, variance: $\sigma^2$)  
- $\sigma^2$ is known.  
\begin{equation}
\text{Sampling distribution: }p(y|\theta) = \frac{1}{\sqrt{2\pi}\sigma}exp(\frac{-1}{2\sigma^2}(y-\theta)^2)
\nonumber
\end{equation}  
  
<br>Conjugate prior and posterior distributions
--------------
The likelihood is an exponential of a quadratic form in $\theta$, so the family of conjugate prior densities looks like  
\begin{equation}
p(\theta) = e^{A\theta^2+B\theta+C}\text{.}
\nonumber
\end{equation}  
Thus, we parameterize this family as 
\begin{equation}
p(\theta) \propto exp(\frac{-1}{2\tau_{0}^2}(\theta-\mu_0)^2)
\nonumber
\end{equation}  
that is $\theta \sim N(\mu_0,\tau_{0}^2)$. $\mu_0 \quad \text{and} \quad \tau_{0}^2$ are hyperparameters and they are known.  
<br>The conjugate prior denstiy implies that the posterior dist for $\theta$ is the exponential of a quadratic form and thus normal, but some algebra is required to reveal its specific form.  
\begin{equation}
p(\theta|y) \propto exp(\frac{-1}{2}(\frac{(y-\theta)^2}{\sigma^2} + \frac{(\theta - \mu_0)^2}{\tau_{0}^2}))
\nonumber
\end{equation}  
이것을 정리하면  
\begin{equation}
p(\theta|y) \propto exp(\frac{-1}{2\tau_{0}^2}(\theta - \mu_{1}^2)),
\nonumber
\end{equation}  
that is $\theta \sim N(\mu_1,\tau_{1}^2)$ where  
\begin{equation}
\mu_{1} = \frac{\frac{1}{\tau_{0}^2}\mu_0 + \frac{1}{\sigma^2}y}{\frac{1}{\tau_{0}^2} + \frac{1}{\sigma^2}} \quad \text{ and } \quad \frac{1}{\tau_{1}^2} = \frac{1}{\tau_{0}^2} + \frac{1}{\sigma^2}\text{.}
\nonumber
\end{equation}  
- The inverse of the variance is called the precision.
- The posterior mean $\mu_1$ is expressed as weighted average of the prior mean and the observed value $y$, with weights proportional to the precisions.
- The posterior precision equals the prior decision plus the data precision.  

<br>We can express $\mu_1$ as the prior mean adjusted toward the observed $y$,  
\begin{equation}
\mu_1 = \mu_0 + (y-\mu_0)\frac{\tau_{0}^2}{\tau_{0}^2+\sigma^2},
\nonumber
\end{equation}  
or as the data shrunk toward the prior mean,  
\begin{equation}
\mu_1 = y - (y-\mu_0)\frac{\sigma^2}{\tau_{0}^2 +\sigma^2}\text{.}
\nonumber
\end{equation}  

<br>
\begin{equation}
\mu_1 = \mu_0 \quad \text{if} \quad y = \mu_0  \quad \text{or} \quad \tau_{0}^2=0
\end{equation} 
\begin{equation}
\mu_1 = y \quad \text{if} \quad y = \mu_0 \quad \text{or} \quad \sigma^2=0
\end{equation}  
(3)번을 보면, $\tau_{0}^2=0$이기 때문에 사전 분포의 분산이 0이라는 것을 의미한다. 따라서 사후 분포와 사전 분포는 같을 것이며, $\mu_0$의 값에 집중될 것이다.  
(4)번을 보면, $\sigma^2=0$이기 때문에 데이터가 완벽히 정확하다는 것을 알 수 있다. 따라서 사후 분포는 관찰된 값인 y에 집중될 것이다.  
만약 $y=\mu_0$이면, prior 평균과 데이터 평균이 일치하기 때문에 사후 평균은 이 지점으로 떨어진다.  

<br>Posterior predictive distribution
---------------------
\begin{equation}
p(\tilde{y}|y) \quad = \int p(\tilde{y}|\theta)p(\theta|y)d\theta \quad \propto \int exp(\frac{-1}{2\sigma^2}(\tilde{y} - \theta)^2) exp(\frac{-1}{2\tau_{1}^2}(\theta - \mu_{1}^2)d\theta \text{.}
\nonumber
\end{equation}  

<br>$E(\tilde{y})|\theta) = \theta$ and $var(\tilde{y}|\theta) = \sigma^2$  
\begin{equation}
E(\tilde{y}|y) = E(E(\tilde{y}|\theta,y)) = E(\theta|y) = \mu_1
\nonumber
\end{equation}  
and  
\begin{equation}
var(\tilde{y}|y) \quad = E(var(\tilde{y}|\theta,y)|y) + var(E(\tilde{y}|\theta,y)|y) \quad = E(\sigma^2|y) + var(\theta|y) \quad  = \sigma^2 + \tau_{1}^2 \text{.}
\nonumber
\end{equation}  

<br>Normal model with multiple observations
-------------
Observations $y = (y_1, ..., y_n)$: i.i.d.  
\begin{equation}
p(\theta|y) \quad \propto p(\theta)p(y|\theta) \quad = p(\theta) \prod_{i=1}^{n} p(y_i|\theta) \quad \propto exp(\frac{-1}{2\tau_{0}^2}(\theta-\mu_0)^2) \prod_{i=1}^{n} \frac{-1}{2\sigma^2}(y_i-\theta)^2   
\nonumber
\end{equation}
\begin{equation}
\propto exp(\frac{-1}{2}(\frac{1}{\tau_{0}^2}(\theta-\mu_0)^2 + \frac{1}{\sigma^2} \sum_{i=1}^{n}(y_i-\theta)^2))
\nonumber
\end{equation}  

<br> Algebraic simplification of this expression shows that the posterior dist depends on y only through the sample mean $\bar{y}$. $\bar{y}$ is a sufficient statistic in this model.  
$\bar{y}|\theta,\sigma^2 \sim N(\theta, \sigma^2/n)$  
\begin{equation}
p(\theta|y_1,...,y_n) = p(\theta|\bar{y}) = N(\theta|\mu_n,\tau_{n}^2)
\nonumber
\end{equation}  
where  
\begin{equation}
\mu_{n} = \frac{\frac{1}{\tau_{0}^2}\mu_0 + \frac{n}{\sigma^2}\bar{y}}{\frac{1}{\tau_{0}^2} + \frac{n}{\sigma^2}} \quad \text{ and } \quad \frac{1}{\tau_{1}^2} = \frac{1}{\tau_{0}^2} + \frac{n}{\sigma^2}\text{.}
\nonumber
\end{equation}  
If n is large, the posterior dist is largely determinded by $\sigma^2$ and the sample value $\bar{y}$.  
As $\tau_0 \rightarrow \infty$ with n fixed, or as $n \rightarrow \infty$ with $\tau_{0}^2$ fixed, we have  
\begin{equation}
p(\theta|y) \approx N(\theta|\bar{y},\sigma^2/n)
\nonumber
\end{equation}
which is a good approximation whenever prior beliefs are relatively diffuse over the range of $\theta$ where the likelihood is important. 

<br>Normal distribution with known mean but unknown variance
-------------
The normal dist with known mean but unknown variance provides an introductory example of the estimation of a scale parameter.  
For $p(y|\theta, \sigma^2) = N(y|\theta, \sigma^2)$, with $\theta$ known and $\sigma^2$ unknown, the likelihood for a vector $y$ of $n$ i.i.d. observations is  
\begin{equation}
p(y|\sigma^2) \propto \sigma^{-n} exp(\frac{1}{2\sigma^2} \sum_{i=1}^{n}(y_i-\theta)^2) = (\sigma^{2})^{-n/2} exp(-\frac{n}{2\sigma^2}v) \text{.}
\nonumber
\end{equation}  
The sufficient statistic is $v = \frac{1}{n}\sum_{i=1}^{n}(y_i-\theta)^2$.  

<br>Using exponential family, we can infer the prior.  
The corresponding conjugate prior density is the inverse-gamma,  
\begin{equation}
p(\sigma^2) \propto (\sigma^2)^{-(\alpha+1)}e^{-\beta/\sigma^2}
\nonumber
\end{equation}
which has hyperparameters $(\alpha, \beta)$.  
 
cf) What is the inverse-$\chi^2$ dist?
A random variable $\theta$ follows inverse-$gamma(\alpha,\beta)$ if its pdf is  
\begin{equation}
p(\theta|\alpha,\beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{-(\alpha+1)}e^{-\beta/\theta} \text{.}
\nonumber
\end{equation}
When $\alpha = v/2, \beta = 1/2$, above is inverse-$\chi^2$ dist.  

<br>A convenient parameterization is as a scaled inverse-$\chi^2$ dist with scale $\sigma_{0}^2$ and $v_{0}$ degrees of freedom. 
The prior dist of $\sigma^2$ is taken to be the dist of $\sigma_{0}^2 v_0/X$, where $X$ is a $\chi_{v_{0}}^2$ random varible.
<span style="color:blue">
\begin{equation}
p(\sigma^2) \sim \text{scaled inverse-} \chi^2(v_{0},\sigma_{0}^2)
\nonumber
\end{equation}
</span>   
The resulting posterior density for $\sigma^2$ is  
\begin{equation} 
p(\sigma^2|y) \propto p(\sigma^2)p(y|\sigma^2) \propto (\sigma^2)^{-((n+v_{0})/2+1)}exp(-\frac{1}{2\sigma^2}(v_{0}\sigma_{0}^2+nv)) \text{.}
\nonumber
\end{equation}
\begin{equation}
\sigma^2|y \sim \text{Inv}-\chi^2(v_{0}+n, \frac{v_{0}\sigma_{0}^2+nv}{v_{0}+n})
\nonumber
\end{equation}
which is a scaled inverse$\chi^2$ dist with scale equal to the degrees-of-freedom-weighted average of the prior data scales and degrees of freedom equal to the sum of the prior and data degrees of freedom.  

<br>Posterior expectation:  
\begin{equation}
E(\sigma^2|y) = \frac{v_{0} \sigma_{0}^2+nv}{v_{0}+n-2} = \frac{v_{0}-2}{v_{0}+n-2}\frac{v_{0} \sigma_{0}^2}{v_{0}-2} + \frac{n}{v_{0}+n-2}v
\nonumber
\end{equation}
\begin{equation}
\frac{v_{0}-2}{v_{0}+n-2} \text{prior expectation} + \frac{n}{v_{0}+n-2}v
\nonumber
\end{equation}
- $v = \frac{1}{n} \sum_{i=1}^{n}(y_i-\theta)^2$  
- $v_0$가 클수록 posterior가 prior에 끌리고, $n$과 $v$가 클수록 posterior가 data에 끌리게 된다.  

<br>Noninformative prior distributions
----------------
사전 분포들이 모집단에 대한 아무런 근거를 가지지 못한다면, 사전 분포들은 구축하기 어려울 수 있으며, 사후 분포에서 최소한의 역할이 보장될 수 있는 사전 분포들을 얻고 싶을 것이다. 이러한 분포들을 'reference prior distributions'라고 부른다. 이 prior density는 모호하고, 평평하며, 널리 퍼지거나 또는 noninformative하다. Noninformative prior dists(무정보 사전 분포)를 쓰는 이유는 '데이터가 스스로 말하도록 하기 위해서'인데, 따라서 추론은 최근 데이터 외의 정보에 영향을 받지 않는다. 쉽게 생각해서 uniform dist 또는 beta(1,1)와 같은 사전 분포가 사후 분포에 아무런 영향을 미치지 않는 것이다. 관련된 개념은 weakly informative prior distribution인데, 이것은 사후 분포를 합법화하는데 충분한 정보를 포함하지만, 드러나지 않은 모수에 대한 과학적인 지식을 완전히 포착하는 시도는 제외한 것이다.  

<br>Proper and improper prior distributions
-------------------------
첫 번째 예제로, 정규 모델에서 분산 $\sigma^2$은 알려져 있지만 평균인 $\theta$에 대해서 추정해야 한다. 앞에서 살펴본 것처럼 $\theta$에 대한 prior는 $N(\mu_0, \tau_{0}^2)$이다. 만약 사전 정확도인 $\frac{1}{\tau_{0}^2}$이 데이터 정확도인 $\frac{n}{\sigma^2}$보다 작아면, 사후 분포는 거의 $\tau_{0}^2 = \infty$인 것이다.  
\begin{equation}
p(\theta|y) \approx N(\theta|\bar{y},n/\sigma^2)
\nonumber
\end{equation}
일반적으로, 우리는 prior density $p(\theta)$를 proper하다고 하는데, 1) 그것이 데이터에 의존하지 않고, 2) 적분했을 때 1이 되어야 한다.   
그런데 앞에서 살펴본 것처럼 분산을 알고 평균을 모르는 경우 $p(\theta)$를 적분하면 무한대가 나오기 때문에 1이 되지 않는다. 따라서 이 예제에서 사전 분포는 improper하지만, 사후 분포는 적어도 하나의 데이터 포인트가 주어지면 proper하게 나오는 것을 알 수 있다.  

<br>Noninformative prior distribution에 대한 두 번째 예제로, 평균은 알지만 분산은 모르는 정규 모델을 생각해 보자. 켤레 사전 분포는 scaled inverse-$\chi^2$ 분포를 따른다. 만약 사전 자유도인 $v_0$가 데이터의 사유도 n 보다 작으면, 사후 분포는 거의 $v_0 = 0$인 것이다. 
\begin{equation}
p(\sigma^2|y) \approx Inv-\chi^2(\sigma^2|n,v)
\nonumber
\end{equation}
위와 같이 사후 분포의 제한된 limiting form은 $\sigma^2$에 대한 prior density를 $p(\sigma^2) \propto 1/\sigma^2$으로 정의함으로써 또한 도출될 수 있는데, $p(\sigma^2)$은 improper하고, 적분하면 무한대가 된다. 하지만 prior와 다르게 posterior는 proper하다.  

<br>Improper prior distributions can lead to proper posterior distributions
------------------
위에서 살펴본 두 예제 중 어느 것도 proper joint probability model인 $p(y, \theta)$를 정의하기 위해서 prior density와 likelihood를 결합하지 않는다. 하지만, 우리는 다음을 통해 베이지안 추론의 대수학과 함께 나아가고, 비정형화된 사후 밀도 함수를 정의할 수 있다. 
\begin{equation}
p(\theta|y) \propto p(y|\theta)p(\theta)
\nonumber
\end{equation}
위의 두 예제에서는 사후 밀도가 proper하다. 왜냐하면 distribution을 적분하면 유한하기 때문이다. Improper prior dists로부터 얻어진 사후 분포들은 좋은 지표로 해석될 수 있는데, 유한한 적분값을 가지고, 합리적인 형태인지 항상 체크해야 한다. Posterior dists의 가장 합리적인 해석은 likelihood가 prior density를 지배하는 상황에서 approximations로 작용하는 것이다.  

<br><br>Introduction to multiparameter models
=========================

<br>Averaging over 'nuisance parameters'
---------------------
Suppose $\theta = (\theta_1, \theta_2)$. We are only interested in inference of for $\theta_1$, so $\theta_2$ may be considered a **nuisance** parameter.  
예를 들어,
\begin{equation}
p(y|\mu,\sigma^2) \sim N(\mu,\sigma^2)
\nonumber
\end{equation}
$\mu$ and $\sigma^2$은 앞에서 살펴본 $\theta_1, \theta_2$처럼 모르는 상태인데, 관심은 주로 $\mu$에 있다.  

<br>우리는 데이터가 주어졌을 때, 관심있는 모수의 conditional dist $= p(\theta_1|y)$에 대해서 찾아야 한다. 다음과 같이 Joint posterior density를 통해서 구할 수 있다. 
\begin{equation}
p(\theta_1, \theta_2|y) \propto p(y|\theta_1, \theta_2)p(\theta_1, \theta_2)
\nonumber
\end{equation}
이것을 $\theta_2$에 대해서 적분하면,
\begin{equation}
p(\theta_1|y) = \int p(\theta_1, \theta_2|y)d\theta_2
\nonumber
\end{equation}
위와 같이 우리가 원하는 $p(\theta_1|y)$을 구할 수 있다.  

<br>또는 다음과 같이 나타낼 수 있다. 
\begin{equation}
p(\theta_1|y) = \int p(\theta_1|\theta_2, y)p(\theta_2|y) d\theta_2
\nonumber
\end{equation}
Conditional posterior dists: ($p(\theta_1|\theta_2, y)$)  
Marginal posterior dist of nuisance parameter: ($p(\theta_2|y)$)  
<br>The posterior dist of interest, $p(\theta_1|y)$, is a mixture of the conditional posterior dists given the nuisance parameter $\theta_2$ where $p(\theta_2|y)$ is a weighting function for the different possible values of $\theta_2$.  
The weights depend on the posterior density of $\theta_2$ and thus on a combinationof evidence from data and prior model. The averaging over nuisance parameters $\theta_2$ can be interpreted generally; for example, $\theta_2$ can include a discrete component representing different possible sub-models.  
<br>위의 적분을 거의 계산하지는 않지만, 이것은 multiparameter models를 구축하고 계산하는데 중요한 전략을 제공한다. 사후 분포는 marginal and conditional simulation에 의해 계산될 수 있다.   
<br>
1) drawing $\theta_2$ from its marginal posterior dist  
2) $\theta_1$ from its conditional posterior dist, given the drawn value of $\theta_2$.  
이 과정을 통해 위의 적분 식을 간접적으로 구할 수 있다.   

<br> Normal data with a noninformative prior distribution
======
$y$: $n$ independent observations from a univariate normal model $N(\mu, \sigma^2)$  
We begin by analyzing the model under a noninformative prior dist.   

<br>A noninformative prior distribution
------------
A sensible vague prior density for $\mu$ and $\sigma$, assuming prior 'independence' of location and scale parameters, is uniform on $(\mu, log\sigma)$ or, equivalently,  
\begin{equation}
p(\mu) \propto 1 \qquad p(\sigma^2) \propto 1
\nonumber
\end{equation}
\begin{equation}
p(\mu, \sigma^2) = p(\mu)p(\sigma^2) \quad \text{독립이라서..} \quad \propto (\sigma^2)^{-1} \text{.}
\nonumber
\end{equation}  

<br>The joint posterior distribution, $p(\mu, \sigma^2|y)$
------------------------
Under this conventional improper prior density, the joint prosterior distribution is proportional to the likelihood function multiplied by the factor $1/sigman^2$.  
\begin{equation}
p(\mu, \sigma^2|y) \propto \sigma^{-n-2} exp(-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-\mu)^2) 
\nonumber
\end{equation}
\begin{equation}
= \sigma^{-n-2} exp(-\frac{1}{2\sigma^2}[\sum_{i=1}^{n}(y_i-\bar{y})^2 + n(\bar{y}-\mu)^2]) 
\nonumber
\end{equation}

\begin{equation}
= \sigma^{-n-2} exp(-\frac{1}{2\sigma^2}[(n-1)s^2 + n(\bar{y}-\mu)^2])  
\nonumber
\end{equation}

where $s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(y_i-\bar{y})^2$. The sufficient statistics are $\bar{y} and s^2$.  

<br>The conditional posterior distribution, $p(\mu|\sigma^2,y)$
-------------
In order to factor the joint density $p(\mu|y)$, we consider first the conditional posterior density $p(\mu|\sigma^2, y)$, and then the marginal posterior density $p(\sigma^2|y)$. To determine $p(\mu|y)$, we use the mean of a normal dist with known variance and a uniform prior dist.  
\begin{equation}
\mu|\sigma^2,y \sim N(\bar{y}, \sigma^2/n)
\nonumber
\end{equation}  

<br>The marginal posterior dist, $p(\sigma^2|y)$
---------------
$p(\sigma^2|y)$을 구하려면 $p(\mu, \sigma^2|y)$를 $\mu$에 대해서 적분하면 된다. 
\begin{equation}
p(\sigma^2|y) \propto \int \sigma^{-n-2} exp(-\frac{1}{2\sigma^2}[(n-1)s^2 + n(\bar{y}-\mu)^2])d\mu
\nonumber
\end{equation}  

$exp(-\frac{1}{2\sigma^2}n(\bar{y}-\mu)^2)$은 simple normal integral이다. 즉,
\begin{equation}
exp(-\frac{1}{2\sigma^2}n(\bar{y}-\mu)^2) = exp(-\frac{(\bar{y}-\mu)^2}{2\frac{\sigma^2}{n}}) \quad \rightarrow  \frac{1}{\sqrt{2\pi \frac{\sigma^2}{n}}}exp(-\frac{(\bar{y}-\mu)^2}{2\frac{\sigma^2}{n}})
\nonumber
\end{equation}
으로 알아볼 수 있다. 따라서
\begin{equation}
p(\sigma^2|y) \propto \sigma^{-n-2} exp(-\frac{1}{2\sigma^2}(n-1)s^2)\sqrt{2\pi \frac{\sigma^2}{n}}
\nonumber
\end{equation}
\begin{equation}
\propto \sigma^{-(n+1)/2} exp(-\frac{1}{2\sigma^2}(n-1)s^2),
\nonumber
\end{equation}
which is a **scaled inverse-$\chi^2$ density**:  
\begin{equation}
\sigma^2|y \sim Inv-\chi^2(n-1,s^2)
\nonumber
\end{equation}  
최종적으로, 우리는 $p(\mu, \sigma^2|y) = p(\mu|\sigma^2,y)p(\sigma^2|y)$을 구할 수 있다.  
여기서 marginal posterior dist $p(\sigma^2|y)$는 샘플링 이론과 유사하다는 것을 알 수 있다. $\sigma^2 \quad (\text{and} \quad \mu)$가 주어졌을 때, 적절히 스케일된 충분통계량 $\frac{(n-1)s^2}{\sigma^2}$의 분포는 $\chi_{n-1}^2$이다.  

<br>Sampling from the joint posterior distribution 
-----------
First, draw $\sigma^2$ from $\sigma^2|y \sim Inv-\chi^2(n-1,s^2)$, them draw $\mu$ from $\mu|\sigma^2,y \sim N(\bar{y}, \sigma^2/n)$  

<br>Analytic form of the marginal posterior distribution of $\mu$
-----------------
모집단 평균인 $\mu$는 관심의 estimand이다. 따라서 베이지안 분석의 목적은 $\mu$의 marginal posterior dist인데, joint posterior dist를 $\sigma^2$에 대해 적분하여 얻을 수 있다. 앞에서 살펴볼 수 있듯이, $\mu$에 대한 사후 분포는 정규 분포와 scaled inverse-$\chi^2$의 결합으로 구성된다. 우리는 다음과 같은 식으로 $\mu$에 대한 marginal posterior dist을 구할 수 있다.
\begin{equation}
p(\mu|y) = \int_{0}^{\infty} p(\mu, \sigma^2|y)d\sigma^2
\nonumber
\end{equation}
위의 적분은 $z = \frac{A}{2\sigma^2}, \quad \text{where} \quad A = (n-1)s^2 + n(\mu-\bar{y})^2$과 같은 치환을 이용하여 구할 수 있다. 결과는 비정규화 감마 pdf를 적분한 것인데, nomalizing constant가 없는 감마 pdf 형태이다. 
\begin{equation}
p(\mu|y) \propto A^{-n/2} \int_{0}^{\infty} z^{(n-2)/2} exp(-z) dz \propto [(n-1)s^2 + n(\mu - \bar{y})^2]^(-n/2)
\nonumber
\end{equation}
\begin{equation}
\propto [1 + \frac{n(\mu-\bar{y})^2}{(n-1)s^2}]^{-n/2}
\nonumber
\end{equation}
This is the $t_{n-1}(\bar{y}, s^2/n)$ density.  

<br>다른 방식으로 생각해보면, noninformative uniform prior dist on $(\mu, log\sigma)$하에서 $\mu$에 대한 사후 분포 $p(\mu|y)$가 다음과 같은 형태를 지닌다는 것을 보인 것이다. 
\begin{equation}
\frac{\bar{y}-\mu}{s/\sqrt{n}}|y \sim t_{n-1}
\nonumber
\end{equation}
여기서 $t_{n-1}$은 location = 0, sce = 1, 자유도 = $n-1$인 표준 t분포이다.  
이러한 marginal posterior dist은 샘플링 이론과의 흥미로운 비교를 제공한다. 샘플링 dist $p(y|\mu, \sigma^2)$하에서, 다음이 성립한다.
\begin{equation}
\frac{\bar{y}-\mu}{s/\sqrt{n}}|\mu, \sigma^2 \sim t_{n-1}
\nonumber
\end{equation}
pivotal quantity $frac{\bar{y}-\mu}{s/\sqrt{n}}$의 샘플링 dist은 nuisance parameter인 $\sigma^2$에 의존하지 않고, 이것의 사후 분포는 데이터에 의존하지 않는다. 일반적으로, estimand의 pivotal quantity는 모든 파라미터와 데이터에 독립인 샘플링 분포를 가지는 데이터와 estimand의 nontrival function으로 정의된다.  

<br>Posterior predictive distribution for a future observation 
----------
Posterior predictive distribution for a future observation $\tilde{y}$ is a t distribution with location $\bar{y}$, scale $(1+\frac{1}{n})^{1/2}$, and $n-1$ degrees of freedom. 위에서 살펴보았던 것과 같은 테크닉을 사용해서 구할 수 있다.
\begin{equation}
p(\tilde{y}|\sigma^2, y) = \int p(\tilde{y}|\mu, \sigma^2, y) p(\mu|\sigma^2, y)d\mu \quad \rightarrow \quad p(\tilde{y}|\sigma^2, y) = N(\tilde{y}|\bar{y},(1+\frac{1}{n})\sigma^2)
\nonumber
\end{equation}

\begin{equation}
p(\tilde{y}|y)\int p(\tilde{y}|\sigma^2, y)d\sigma^2
\nonumber
\end{equation}
